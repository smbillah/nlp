Programming Project Evaluation Form Student Name: Project: Date: Correctness Criteria: ¥ Reads normal input data ¥ Handles incorrect input data ¥ Calculates correct results ¥ Outputs results properly ¥ Prints appropriate error messages Score:______/ 50 Design Criteria: ¥ Problem decomposition ¥ Choice of data structures ¥ Choice of algorithms ¥ Program efficiency (space/time) Score:______/ 20 Documentation Criteria: ¥ Comments for classes and methods ¥ Comments describing algorithms ¥ Comments describing data structures ¥ Description of program design ¥ Description of test results ¥ Description of known problems Score:______/ 10 Style Criteria: ¥ Method and variable names ¥ Program indenting ¥ Use of white space ¥ Ordering of methods ¥ Easy to read code Score:______/ 10 Testing Criteria: ¥ Normal input data ¥ Incorrect input data ¥ Special cases for data structures ¥ Special cases for algorithms Score:______/ 10 Grader Comments: Total:______/ 100 Installation Programming language: Python Source code location: /home/sbillah/nlp/ Documentation: sbillah@turing:~/nlp$ ./NLPEngine.py -h usage: NLPEngine.py [-h] [-o DST_DIR] [-s SRC_DIR] [-nlp BIGRAMS] This is Syed's NLP program for HW01 optional arguments: -h, --help show this help message and exit -o DST_DIR, --output DST_DIR The directory where output goes -s SRC_DIR, --source SRC_DIR The directory where raw files reside -nlp BIGRAMS, --count_bigrams BIGRAMS count the bigrams in src directory and write to the dst folder in descending order Here is a complete command line example: sbillah@turing:~/nlp$ ./NLPEngine.py -s /home/sgauch/public_html/5013IR/files/ -o parsed/ -nlp bigram_counts Done! AlgorithmI use an in-memory Hash-table to count all bigrams. The pseudo code is given bellow: initialize hash-table ht<(tuple), int> foreach file f in input_directory: pain_text = html_parser(f.read()) tokens = tokenizer(plain_text) i = 0 foreach token t in tokens: if i>0: ht[(t[i-1],t[i])] += 1 i++ sort ht write ht to file Time Complexity: N = num_files M= avg num_of_words_per_file Bigram generation complexity: O(N*M) Hash-table sorting complexity: O(N*M*log(N*M)) Total complexity: O(N*M) + O(N*M*log(N*M)) = O(N*M*log(N*M)) ParserConfigurationHere is the configuration of my html parser and tokenizer: str_src_dir /home/sgauch/public_html/5013IR/files/ str_dst_dir parsed/ str_doc_id_file_name bigram.txt min_token_freq 3 max_token_freq 1000 min_token_len 3 max_token_len 12 str_stop_list Stoplist from this link: http://www.csce.uark.edu/~sgauch/5013IR/S12/index.html Runtime&MemoryusageInputsize(#files)Runtime(sec)Memorysize(MB)Totalbigrams1005.6011055,01320019.21210116,69730030.31277166,17950554.64440272,646 Top50bigramsrisksjul607netalter344alterdynip340healthcare215papertitle208cominterramp204netsunbelt189edupsu177netmci152edunodak144eduuiuc142criticalanalysis142massmedia141eduumich137netidt133eduumn132milnavy130politicalscience129rightsreserved127eduarizona120humanrights117eduindiana115socialsecurity111hogynem111eduutexas111nemzetmagyar110losangeles109horngyula109sendcomments108mcicampus108blackstudies107worldwar106homepage106bookreport106termpapers105httpwww105writtenprice104urbanstudies104termpapercom104termpaper104subjectindex104sportsrecreation104specificpaper104paperwritten104paperclick104descriptionpaper104copyrightasm104commentstermpaper104commentscomments104coldsurges104Bottom20bigramsabacskiskun1ababaresponse1aauzoo1aaupsy1aauhum1aasnearly1aarpnational1aarpamerican1aaronword1aaronnetland1aaronmoshiashwili1aaronjon1aaronhappened1aaroncomparison1aalenimage1aaemassagosem1aaeliberalisdemokraciato1aachenrad1aachenoph1aaapassed1 NLPEngine1#!/usr/bin/envpython23'''4CreatedonFeb23,201256@author:Masum7'''89frommyparserimportBiGram10frommyparserimportsrc_dir,dst_dir11frommycollectionimportargparse1213classNLPEngine():14config={}1516def__init__(self):17self.read_config()1819defread_config(self,name="config.txt"):4041defbuild_bigram_index(self):42_=BiGram(self.config)4344#importargparse45if__name__=='__main__':46args=argparse.ArgumentParser(description="ThisisSyed'sNLPprogram")47args.add_argument("o","output",dest="dst_dir",48help="Thedirectorywhereoutputgoes",default="")49args.add_argument("s","source",dest="src_dir",50help="Thedirectorywhererawfilesreside",default="")5152args.add_argument("nlp","count_bigrams",dest="bigrams",53help="Reindexofallfilesandoverwriteexistingdictandpostfiles,i.e.iall",default="")545556args=args.parse_args()57src_dir=args.src_dir58dst_dir=args.dst_dir5960ifnotargs.bigrams:61print'InvalidorNoarguments.Trywithhforhelp!!'62else:63nlp=NLPEngine()64ifargs.bigrams:nlp.build_bigram_index()65print'Done!'6667#nlp=SearchEngine()68#nlp.search_query('.8susan.1uark.1edu',True)69#print'Done!'7071Page1 shared1'''2CreatedonMar11,201234@author:Masum5'''67importre8fromcollectionsimportdefaultdict910#============globalregex=============================11is_word=re.compile("[azAZ_]+$")12delim=re.compile("|\t|\.||:|\@|\\|/|,|;|\"|!|\*]+")1314#================path==============================15dst_dir=''16src_dir=''1718#============globalhashvariables=====================19doc_id=defaultdict(int)20term_count=defaultdict(int)2122defparse(line,config):23line=line.strip()24ifnotline:return[]2526tokens=[]27forwordindelim.split(line):28word=word.strip("~`$%^&()[]|<>=+_/").lower()29ifwordinconfig['str_stop_list']:continue30ifconfig['min_token_len']<=len(word)<=config['max_token_len']andis_word.match(word):31tokens.append(word)32returntokensPage1 BiGram1'''2CreatedonFeb4,201334@author:Masum5'''6importos78fromBiGramHTMLParserimportBiGramHTMLParser9fromcollectionsimportdefaultdict1011classBiGram():12def__init__(self,config):13self.config=config14self.ht=defaultdict(int)15self.htmlparser=BiGramHTMLParser(self.config,self.ht)16self.start_batch_processing()17self.write_file_map()1819defstart_batch_processing(self):20file_id=02122forin_fileinos.listdir(self.config['str_src_dir']):23#ifin_filenotin['medium.html','simple.html']:continue#fortesting24withopen(self.config['str_src_dir']+in_file,'r')asf:25self.htmlparser.feed(f.read(),file_id)26file_id+=12728defwrite_file_map(self):29#writingbigramfiletoafilenamedunderdocumentid30withopen(self.config['str_dst_dir']+self.config['str_doc_id_file_name'],'wb+')asf:31forwords,countinsorted(self.ht.iteritems(),key=lambda(k,v):(v,k),reverse=True):32f.write(words[0]+""+words[1]+""+str(count)+"\n")33Page1 BiGramHTMLParser1'''2CreatedonMar11,201234@author:Masum5'''6importsys7fromHTMLParserimportHTMLParser8fromsharedimportparse91011classBiGramHTMLParser(HTMLParser):12text,N,newline='',0,{'br':'\n','BR':'\n'}1314def__init__(self,config,ht):15HTMLParser.__init__(self)16self.config=config17self.ht=ht181920defhandle_data(self,raw):21self.text=self.text+raw+self.newline.get(self.lasttag,'')2223#formatis(token=>value,hereval)24deffeed(self,data,did):25#extractinghtmltext26try:27HTMLParser.feed(self,data)28except:29sys.exc_clear()3031#tokenizingextracteddata32forlineinself.text.splitlines():33tokens=parse(line,self.config)34ifnottokens:continue35foriinxrange(len(tokens)):36ifi>0:37self.ht[(tokens[i],tokens[i1])]+=1;38self.N+=13940#cleaningfornextfeed41self.text='';self.N=042HTMLParser.reset(self)4344Page1